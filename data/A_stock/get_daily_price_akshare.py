"""
ä½¿ç”¨ AKShare è·å– A è‚¡æ—¥çº¿æ•°æ®

æ”¯æŒå¢é‡æ›´æ–°ï¼šæ£€æŸ¥å·²å­˜åœ¨çš„æ•°æ®ï¼Œåªè·å–æ–°æ—¥æœŸçš„æ•°æ®ã€‚
è¾“å‡ºæ ¼å¼å…¼å®¹ merge_jsonl.py è½¬æ¢è„šæœ¬ã€‚
"""

import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd

from data_source import create_data_source

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


def get_latest_date_from_csv(csv_path: Path) -> Optional[str]:
    """ä»ç°æœ‰ CSV æ–‡ä»¶è·å–æœ€æ–°æ—¥æœŸ

    Args:
        csv_path: CSV æ–‡ä»¶è·¯å¾„

    Returns:
        æœ€æ–°æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDD)ï¼Œæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºè¿”å› None
    """
    if not csv_path.exists():
        return None

    try:
        df = pd.read_csv(csv_path)
        if df.empty or "trade_date" not in df.columns:
            return None

        # ç¡®ä¿ trade_date æ˜¯å­—ç¬¦ä¸²æ ¼å¼
        df["trade_date"] = df["trade_date"].astype(str)
        latest_date = df["trade_date"].max()
        return latest_date
    except Exception as e:
        logger.warning(f"è¯»å– CSV è·å–æœ€æ–°æ—¥æœŸå¤±è´¥: {e}")
        return None


def _get_db_manager():
    """è·å–æ•°æ®åº“ç®¡ç†å™¨ï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼‰"""
    import sys
    project_root = Path(__file__).parent.parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))

    from data.database.connection import DatabaseManager
    return DatabaseManager


def get_latest_date_from_duckdb() -> Optional[str]:
    """ä» DuckDB è·å–è‚¡ç¥¨æ—¥çº¿æœ€æ–°æ—¥æœŸ

    Returns:
        æœ€æ–°æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDD)ï¼Œæ— æ•°æ®è¿”å› None
    """
    try:
        DatabaseManager = _get_db_manager()

        with DatabaseManager() as db:
            if not db.table_exists("stock_daily_prices"):
                return None

            df = db.query(
                "SELECT MAX(trade_date) as max_date FROM stock_daily_prices WHERE market = 'cn'"
            )
            if not df.empty and df.iloc[0]["max_date"] is not None:
                # è½¬æ¢ä¸º YYYYMMDD æ ¼å¼
                max_date = df.iloc[0]["max_date"]
                if isinstance(max_date, str) and "-" in max_date:
                    return max_date.replace("-", "")
                if hasattr(max_date, 'strftime'):
                    return max_date.strftime("%Y%m%d")
                return str(max_date).replace("-", "")
        return None
    except Exception as e:
        logger.warning(f"ä» DuckDB è·å–è‚¡ç¥¨æœ€æ–°æ—¥æœŸå¤±è´¥: {e}")
        return None


def get_latest_index_date_from_duckdb(index_code: str = "000016.SH") -> Optional[str]:
    """ä» DuckDB è·å–æŒ‡æ•°æ—¥çº¿æœ€æ–°æ—¥æœŸ

    Args:
        index_code: æŒ‡æ•°ä»£ç 

    Returns:
        æœ€æ–°æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDD)ï¼Œæ— æ•°æ®è¿”å› None
    """
    try:
        DatabaseManager = _get_db_manager()

        with DatabaseManager() as db:
            if not db.table_exists("index_daily_prices"):
                return None

            df = db.query(
                "SELECT MAX(trade_date) as max_date FROM index_daily_prices WHERE index_code = ?",
                (index_code,)
            )
            if not df.empty:
                max_date = df.iloc[0]["max_date"]
                # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå€¼ï¼ˆNone æˆ– NaTï¼‰
                if max_date is None or pd.isna(max_date):
                    return None
                # è½¬æ¢ä¸º YYYYMMDD æ ¼å¼
                if isinstance(max_date, str) and "-" in max_date:
                    return max_date.replace("-", "")
                # å¤„ç† date å¯¹è±¡
                if hasattr(max_date, 'strftime'):
                    return max_date.strftime("%Y%m%d")
                return str(max_date).replace("-", "")
        return None
    except Exception as e:
        logger.warning(f"ä» DuckDB è·å–æŒ‡æ•°æœ€æ–°æ—¥æœŸå¤±è´¥: {e}")
        return None


def save_index_daily_to_duckdb(df: pd.DataFrame, index_code: str) -> bool:
    """å°†æŒ‡æ•°æ—¥çº¿æ•°æ®ä¿å­˜åˆ° DuckDB

    Args:
        df: æŒ‡æ•°æ—¥çº¿ DataFrame
        index_code: æŒ‡æ•°ä»£ç 

    Returns:
        True å¦‚æœä¿å­˜æˆåŠŸ
    """
    if df.empty:
        return False

    try:
        DatabaseManager = _get_db_manager()

        with DatabaseManager() as db:
            # ç¡®ä¿è¡¨å­˜åœ¨
            from data.database.models import create_table
            create_table("index_daily_prices")

            # å‡†å¤‡ DataFrame ç”¨äºæ’å…¥
            df_insert = df.copy()
            df_insert["index_code"] = index_code
            df_insert["trade_date"] = df_insert["trade_date"].astype(str).apply(
                lambda x: f"{x[:4]}-{x[4:6]}-{x[6:]}" if len(x) == 8 and "-" not in x else x
            )

            # é‡å‘½åå’Œé€‰æ‹©åˆ—
            col_mapping = {"vol": "volume"}
            df_insert = df_insert.rename(columns=col_mapping)

            # ç¡®ä¿å¿…éœ€åˆ—å­˜åœ¨
            required_cols = ["index_code", "trade_date", "open", "high", "low", "close", "volume"]
            for col in required_cols:
                if col not in df_insert.columns:
                    df_insert[col] = None

            if "amount" not in df_insert.columns:
                df_insert["amount"] = None

            # åªä¿ç•™éœ€è¦çš„åˆ—
            df_insert = df_insert[["index_code", "trade_date", "open", "high", "low", "close", "volume", "amount"]]

            # è·å–è¦åˆ é™¤çš„æ—¥æœŸåˆ—è¡¨
            dates_to_delete = df_insert["trade_date"].unique().tolist()

            # æ‰¹é‡åˆ é™¤å·²å­˜åœ¨çš„è®°å½•
            if dates_to_delete:
                placeholders = ", ".join(["?" for _ in dates_to_delete])
                db.execute(
                    f"DELETE FROM index_daily_prices WHERE index_code = ? AND trade_date IN ({placeholders})",
                    (index_code, *dates_to_delete)
                )

            # ä½¿ç”¨ insert_df æ‰¹é‡æ’å…¥
            db.insert_df("index_daily_prices", df_insert)

            logger.info(f"å·²ä¿å­˜ {len(df_insert)} æ¡æŒ‡æ•°æ—¥çº¿æ•°æ®åˆ° DuckDB")
            return True

    except Exception as e:
        logger.error(f"ä¿å­˜æŒ‡æ•°æ—¥çº¿æ•°æ®åˆ° DuckDB å¤±è´¥: {e}")
        return False


def calculate_start_date(existing_latest: Optional[str], default_start: str) -> Tuple[str, bool]:
    """è®¡ç®—æ•°æ®è·å–çš„èµ·å§‹æ—¥æœŸ

    Args:
        existing_latest: ç°æœ‰æ•°æ®çš„æœ€æ–°æ—¥æœŸ
        default_start: é»˜è®¤èµ·å§‹æ—¥æœŸ

    Returns:
        (èµ·å§‹æ—¥æœŸ, æ˜¯å¦éœ€è¦æ›´æ–°)
    """
    today = datetime.now().strftime("%Y%m%d")

    if existing_latest is None:
        return default_start, True

    # å¦‚æœç°æœ‰æ•°æ®å·²ç»æ˜¯ä»Šå¤©æˆ–ä¹‹åï¼Œæ— éœ€æ›´æ–°
    if existing_latest >= today:
        return existing_latest, False

    # ä»ç°æœ‰æœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€å¤©å¼€å§‹è·å–
    try:
        latest_dt = datetime.strptime(existing_latest, "%Y%m%d")
        next_day = (latest_dt + timedelta(days=1)).strftime("%Y%m%d")
        return next_day, True
    except ValueError:
        return default_start, True


def get_daily_price_a_stock(
    index_code: str = "000016.SH",
    output_dir: Optional[Path] = None,
    daily_start_date: str = "20250101",
    fallback_csv: Optional[Path] = None,
    force_update: bool = False,
) -> Optional[pd.DataFrame]:
    """è·å– A è‚¡æŒ‡æ•°æˆåˆ†è‚¡æ—¥çº¿æ•°æ®ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰

    ä½¿ç”¨ AKShare è·å–æŒ‡å®šæŒ‡æ•°çš„æˆåˆ†è‚¡æ—¥çº¿æ•°æ®ã€‚
    å¦‚æœå·²æœ‰æ•°æ®ï¼Œåªè·å–æ–°æ—¥æœŸçš„æ•°æ®å¹¶åˆå¹¶ã€‚

    Args:
        index_code: æŒ‡æ•°ä»£ç ï¼Œé»˜è®¤ä¸Šè¯50 (000016.SH)
        output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º ./A_stock_data
        daily_start_date: æ•°æ®å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDDï¼ˆä»…é¦–æ¬¡è·å–æ—¶ä½¿ç”¨ï¼‰
        fallback_csv: æˆåˆ†è‚¡åˆ—è¡¨å¤‡ç”¨ CSV æ–‡ä»¶è·¯å¾„
        force_update: å¼ºåˆ¶å…¨é‡æ›´æ–°ï¼Œå¿½ç•¥ç°æœ‰æ•°æ®

    Returns:
        DataFrame åŒ…å«æ—¥çº¿æ•°æ®ï¼Œå¤±è´¥è¿”å› None
    """
    # è®¾ç½®è¾“å‡ºç›®å½•å’Œæ–‡ä»¶è·¯å¾„
    if output_dir is None:
        output_dir = Path(__file__).parent / "A_stock_data"
    else:
        output_dir = Path(output_dir)

    output_dir.mkdir(parents=True, exist_ok=True)

    index_name = "sse_50" if index_code == "000016.SH" else index_code.replace(".", "_")
    daily_file = output_dir / f"daily_prices_{index_name}.csv"

    # æ£€æŸ¥ç°æœ‰æ•°æ®ï¼Œå†³å®šèµ·å§‹æ—¥æœŸ
    existing_latest = None
    if not force_update:
        existing_latest = get_latest_date_from_csv(daily_file)
        if existing_latest is None:
            existing_latest = get_latest_date_from_duckdb()

    start_date, need_update = calculate_start_date(existing_latest, daily_start_date)
    daily_end_date = datetime.now().strftime("%Y%m%d")

    if not need_update:
        print(f"âœ… æ•°æ®å·²æ˜¯æœ€æ–° (æœ€æ–°æ—¥æœŸ: {existing_latest})ï¼Œæ— éœ€æ›´æ–°")
        # è¿”å›ç°æœ‰æ•°æ®
        if daily_file.exists():
            return pd.read_csv(daily_file)
        return None

    print(f"ğŸ“Š å¢é‡æ›´æ–°: {start_date} - {daily_end_date} (ç°æœ‰æœ€æ–°: {existing_latest or 'æ— '})")

    # åˆ›å»º AKShare æ•°æ®æº
    source = create_data_source("akshare")

    try:
        # 1. è·å–æŒ‡æ•°æˆåˆ†è‚¡
        print(f"æ­£åœ¨è·å–æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®: {index_code}")
        df_cons = source.get_index_constituents(index_code)

        # å¦‚æœ API è¿”å›ç©ºæ•°æ®ï¼Œå°è¯•è¯»å–å¤‡ç”¨æ–‡ä»¶
        if df_cons.empty:
            if fallback_csv and Path(fallback_csv).exists():
                print(f"API è¿”å›ç©ºæ•°æ®ï¼Œä½¿ç”¨å¤‡ç”¨æ–‡ä»¶: {fallback_csv}")
                df_cons = pd.read_csv(fallback_csv)
            else:
                print(f"æœªè·å–åˆ°æŒ‡æ•° {index_code} çš„æˆåˆ†è‚¡æ•°æ®")
                return None

        # æå–å”¯ä¸€çš„æˆåˆ†è‚¡ä»£ç 
        code_list = df_cons["con_code"].unique().tolist()
        num_stocks = len(code_list)
        print(f"å…± {num_stocks} åªæˆåˆ†è‚¡")

        # 1.5 æ·»åŠ æŒä»“ä¸­çš„è‚¡ç¥¨ï¼ˆå³ä½¿å·²ä»æŒ‡æ•°ä¸­å‰”é™¤ï¼‰
        try:
            from validate_data import DataValidator
            validator = DataValidator(output_dir)
            held_stocks = validator.get_all_held_stocks("daily")
            # æ‰¾å‡ºæŒä»“ä¸­ä½†ä¸åœ¨æˆåˆ†è‚¡ä¸­çš„è‚¡ç¥¨
            extra_held = held_stocks - set(code_list)
            if extra_held:
                print(f"ğŸ“Š æ·»åŠ  {len(extra_held)} åªæŒä»“è‚¡ç¥¨ï¼ˆå·²ä»æŒ‡æ•°å‰”é™¤ï¼‰: {sorted(extra_held)}")
                code_list = list(set(code_list) | extra_held)
        except Exception as e:
            logger.warning(f"è·å–æŒä»“è‚¡ç¥¨å¤±è´¥: {e}")

        # 2. è·å–æ—¥çº¿æ•°æ®
        print(f"æ­£åœ¨è·å–æ—¥çº¿æ•°æ®: {start_date} - {daily_end_date}")
        df_new = source.get_stock_daily(code_list, start_date, daily_end_date)

        if df_new.empty:
            print("æœªè·å–åˆ°æ–°çš„æ—¥çº¿æ•°æ®")
            if daily_file.exists():
                return pd.read_csv(daily_file)
            return None

        # 3. åˆå¹¶æ•°æ®
        if daily_file.exists() and not force_update:
            print("åˆå¹¶ç°æœ‰æ•°æ®ä¸æ–°æ•°æ®...")
            df_existing = pd.read_csv(daily_file)
            # ç¡®ä¿ trade_date æ ¼å¼ä¸€è‡´
            df_existing["trade_date"] = df_existing["trade_date"].astype(str)
            df_new["trade_date"] = df_new["trade_date"].astype(str)

            # åˆå¹¶å¹¶å»é‡ï¼ˆä»¥ ts_code + trade_date ä¸ºé”®ï¼‰
            df_combined = pd.concat([df_existing, df_new], ignore_index=True)
            df_combined = df_combined.drop_duplicates(
                subset=["ts_code", "trade_date"],
                keep="last"  # ä¿ç•™æ–°æ•°æ®
            )
            df_combined = df_combined.sort_values(["ts_code", "trade_date"])
            df_daily = df_combined
            print(f"åˆå¹¶åè®°å½•æ•°: {len(df_daily)} (æ–°å¢: {len(df_new)})")
        else:
            df_daily = df_new

        # 4. ä¿å­˜æ•°æ®
        df_daily.to_csv(daily_file, index=False, encoding="utf-8")
        print(f"âœ… æ•°æ®å·²ä¿å­˜: {daily_file} (shape: {df_daily.shape})")

        return df_daily

    except Exception as e:
        print(f"è·å–æ•°æ®å¤±è´¥: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        return None


def convert_index_daily_to_json(
    df: pd.DataFrame,
    symbol: str = "000016.SH",
    output_file: Optional[Path] = None,
) -> Dict:
    """å°†æŒ‡æ•°æ—¥çº¿æ•°æ®è½¬æ¢ä¸º JSON æ ¼å¼ï¼ˆä¸ Alpha Vantage æ ¼å¼å…¼å®¹ï¼‰

    Args:
        df: æŒ‡æ•°æ—¥çº¿ DataFrame
        symbol: æŒ‡æ•°ä»£ç 
        output_file: è¾“å‡º JSON æ–‡ä»¶è·¯å¾„

    Returns:
        JSON æ ¼å¼çš„æ•°æ®å­—å…¸
    """
    if df.empty:
        print("è­¦å‘Š: DataFrame ä¸ºç©º")
        return {}

    # æŒ‰æ—¥æœŸé™åºæ’åˆ—
    df = df.sort_values(by="trade_date", ascending=False).reset_index(drop=True)

    # è·å–æœ€åæ›´æ–°æ—¥æœŸ
    last_refreshed = str(df.iloc[0]["trade_date"])
    last_refreshed_formatted = f"{last_refreshed[:4]}-{last_refreshed[4:6]}-{last_refreshed[6:]}"

    # æ„å»º JSON ç»“æ„
    json_data = {
        "Meta Data": {
            "1. Information": "Daily Prices (open, high, low, close) and Volumes",
            "2. Symbol": symbol,
            "3. Last Refreshed": last_refreshed_formatted,
            "4. Output Size": "Compact",
            "5. Time Zone": "Asia/Shanghai",
        },
        "Time Series (Daily)": {},
    }

    # è½¬æ¢æ¯ä¸€è¡Œæ•°æ®
    for _, row in df.iterrows():
        trade_date = str(row["trade_date"])
        date_formatted = f"{trade_date[:4]}-{trade_date[4:6]}-{trade_date[6:]}"

        json_data["Time Series (Daily)"][date_formatted] = {
            "1. open": f"{row['open']:.4f}",
            "2. high": f"{row['high']:.4f}",
            "3. low": f"{row['low']:.4f}",
            "4. close": f"{row['close']:.4f}",
            "5. volume": str(int(row["vol"])) if pd.notna(row["vol"]) else "0",
        }

    # ä¿å­˜åˆ°æ–‡ä»¶
    if output_file:
        output_file = Path(output_file)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=4, ensure_ascii=False)
        print(f"JSON æ•°æ®å·²ä¿å­˜: {output_file}")

    return json_data


def get_latest_index_date_from_json(json_path: Path) -> Optional[str]:
    """ä»ç°æœ‰ JSON æ–‡ä»¶è·å–æœ€æ–°æ—¥æœŸ

    Args:
        json_path: JSON æ–‡ä»¶è·¯å¾„

    Returns:
        æœ€æ–°æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDD)ï¼Œæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºè¿”å› None
    """
    if not json_path.exists():
        return None

    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        time_series = data.get("Time Series (Daily)", {})
        if not time_series:
            return None

        # è·å–æ‰€æœ‰æ—¥æœŸå¹¶æ‰¾æœ€å¤§å€¼
        dates = list(time_series.keys())
        if not dates:
            return None

        # æ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DDï¼Œè½¬æ¢ä¸º YYYYMMDD
        latest_date = max(dates)
        return latest_date.replace("-", "")
    except Exception as e:
        logger.warning(f"è¯»å– JSON è·å–æœ€æ–°æ—¥æœŸå¤±è´¥: {e}")
        return None


def get_index_daily_data(
    index_code: str = "000016.SH",
    start_date: str = "20250101",
    end_date: Optional[str] = None,
    output_dir: Optional[Path] = None,
    force_update: bool = False,
) -> Optional[pd.DataFrame]:
    """è·å–æŒ‡æ•°æ—¥çº¿æ•°æ®ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰

    ä½¿ç”¨ AKShare è·å–æŒ‡æ•°æ—¥çº¿æ•°æ®ï¼Œæ”¯æŒå¢é‡æ›´æ–°ã€‚
    å¦‚æœå·²æœ‰æ•°æ®ï¼Œåªè·å–æ–°æ—¥æœŸçš„æ•°æ®å¹¶åˆå¹¶ã€‚
    åŒæ—¶ä¿å­˜åˆ° JSON æ–‡ä»¶å’Œ DuckDBã€‚

    Args:
        index_code: æŒ‡æ•°ä»£ç ï¼Œé»˜è®¤ä¸Šè¯50 (000016.SH)
        start_date: å¼€å§‹æ—¥æœŸ YYYYMMDDï¼ˆä»…é¦–æ¬¡è·å–æ—¶ä½¿ç”¨ï¼‰
        end_date: ç»“æŸæ—¥æœŸ YYYYMMDDï¼Œé»˜è®¤ä»Šå¤©
        output_dir: è¾“å‡ºç›®å½•
        force_update: å¼ºåˆ¶å…¨é‡æ›´æ–°ï¼Œå¿½ç•¥ç°æœ‰æ•°æ®

    Returns:
        DataFrame åŒ…å«æŒ‡æ•°æ—¥çº¿æ•°æ®ï¼Œå¤±è´¥è¿”å› None
    """
    # è®¾ç½®è¾“å‡ºç›®å½•
    if output_dir is None:
        output_dir = Path(__file__).parent
    else:
        output_dir = Path(output_dir)

    output_dir.mkdir(parents=True, exist_ok=True)

    index_name = "sse_50" if index_code == "000016.SH" else index_code.replace(".", "_")
    json_file = output_dir / f"index_daily_{index_name}.json"

    # æ£€æŸ¥ç°æœ‰æ•°æ®ï¼Œå†³å®šèµ·å§‹æ—¥æœŸ
    existing_latest = None
    if not force_update:
        # ä¼˜å…ˆæ£€æŸ¥ DuckDB
        existing_latest = get_latest_index_date_from_duckdb(index_code)
        # å¤‡ç”¨æ£€æŸ¥ JSON æ–‡ä»¶
        if existing_latest is None:
            existing_latest = get_latest_index_date_from_json(json_file)

    actual_start_date, need_update = calculate_start_date(existing_latest, start_date)

    # é»˜è®¤ç»“æŸæ—¥æœŸä¸ºä»Šå¤©
    if end_date is None:
        end_date = datetime.now().strftime("%Y%m%d")

    if not need_update:
        print(f"âœ… æŒ‡æ•°æ•°æ®å·²æ˜¯æœ€æ–° (æœ€æ–°æ—¥æœŸ: {existing_latest})ï¼Œæ— éœ€æ›´æ–°")
        # è¿”å›ç°æœ‰æ•°æ®ï¼ˆä» JSON è¯»å–ï¼‰
        if json_file.exists():
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                time_series = data.get("Time Series (Daily)", {})
                if time_series:
                    records = []
                    for date_str, values in time_series.items():
                        records.append({
                            "trade_date": date_str.replace("-", ""),
                            "open": float(values["1. open"]),
                            "high": float(values["2. high"]),
                            "low": float(values["3. low"]),
                            "close": float(values["4. close"]),
                            "vol": int(values["5. volume"]),
                        })
                    return pd.DataFrame(records)
            except Exception as e:
                logger.warning(f"è¯»å–ç°æœ‰ JSON æ•°æ®å¤±è´¥: {e}")
        return None

    print(f"ğŸ“Š æŒ‡æ•°å¢é‡æ›´æ–°: {actual_start_date} - {end_date} (ç°æœ‰æœ€æ–°: {existing_latest or 'æ— '})")

    # åˆ›å»º AKShare æ•°æ®æº
    source = create_data_source("akshare")

    try:
        df_new = source.get_index_daily(index_code, actual_start_date, end_date)

        if df_new.empty:
            print("æœªè·å–åˆ°æ–°çš„æŒ‡æ•°æ—¥çº¿æ•°æ®")
            return None

        # åˆå¹¶ç°æœ‰æ•°æ®
        df_combined = df_new
        if json_file.exists() and not force_update:
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                time_series = data.get("Time Series (Daily)", {})
                if time_series:
                    existing_records = []
                    for date_str, values in time_series.items():
                        existing_records.append({
                            "trade_date": date_str.replace("-", ""),
                            "open": float(values["1. open"]),
                            "high": float(values["2. high"]),
                            "low": float(values["3. low"]),
                            "close": float(values["4. close"]),
                            "vol": int(values["5. volume"]),
                        })
                    df_existing = pd.DataFrame(existing_records)

                    # åˆå¹¶å¹¶å»é‡
                    df_new["trade_date"] = df_new["trade_date"].astype(str)
                    df_existing["trade_date"] = df_existing["trade_date"].astype(str)
                    df_combined = pd.concat([df_existing, df_new], ignore_index=True)
                    df_combined = df_combined.drop_duplicates(subset=["trade_date"], keep="last")
                    df_combined = df_combined.sort_values("trade_date")
                    print(f"åˆå¹¶åæŒ‡æ•°è®°å½•æ•°: {len(df_combined)} (æ–°å¢: {len(df_new)})")
            except Exception as e:
                logger.warning(f"åˆå¹¶ç°æœ‰æ•°æ®å¤±è´¥: {e}")

        # ä¿å­˜ä¸º JSON
        convert_index_daily_to_json(df_combined, symbol=index_code, output_file=json_file)

        # ä¿å­˜åˆ° DuckDB
        if save_index_daily_to_duckdb(df_new, index_code):
            print(f"âœ… æŒ‡æ•°æ•°æ®å·²ä¿å­˜åˆ° DuckDB")

        return df_combined

    except Exception as e:
        print(f"è·å–æŒ‡æ•°æ—¥çº¿æ•°æ®å¤±è´¥: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        return None


def update_weight_file(
    index_code: str = "000016.SH",
    output_dir: Optional[Path] = None,
) -> bool:
    """ä» API æ›´æ–°æœ¬åœ°æˆåˆ†è‚¡æƒé‡æ–‡ä»¶

    ç¡®ä¿æœ¬åœ°å¤‡ç”¨æ–‡ä»¶ä¸æœ€æ–°æŒ‡æ•°æˆåˆ†ä¿æŒä¸€è‡´ã€‚

    Args:
        index_code: æŒ‡æ•°ä»£ç 
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        True å¦‚æœæ›´æ–°æˆåŠŸ
    """
    if output_dir is None:
        output_dir = Path(__file__).parent / "A_stock_data"

    index_name = "sse_50" if index_code == "000016.SH" else index_code.replace(".", "_")
    weight_file = output_dir / f"{index_name}_weight.csv"

    source = create_data_source("akshare")
    df_cons = source.get_index_constituents(index_code)

    if df_cons.empty:
        logger.warning("ä» API è·å–æˆåˆ†è‚¡å¤±è´¥ï¼Œæƒé‡æ–‡ä»¶æœªæ›´æ–°")
        return False

    # è¯»å–ç°æœ‰æ–‡ä»¶ä»¥ä¿ç•™è‚¡ç¥¨åç§°
    stock_name_map = {}
    if weight_file.exists():
        try:
            old_df = pd.read_csv(weight_file)
            if "stock_name" in old_df.columns:
                stock_name_map = dict(zip(old_df["con_code"], old_df["stock_name"]))
        except Exception as e:
            logger.warning(f"è¯»å–ç°æœ‰æƒé‡æ–‡ä»¶å¤±è´¥: {e}")

    # æ·»åŠ è‚¡ç¥¨åç§°
    df_cons["stock_name"] = df_cons["con_code"].apply(
        lambda x: stock_name_map.get(x, df_cons[df_cons["con_code"] == x]["con_name"].iloc[0]
                                      if "con_name" in df_cons.columns and len(df_cons[df_cons["con_code"] == x]) > 0
                                      else "Unknown")
    )

    # æ·»åŠ  index_code åˆ—
    df_cons["index_code"] = index_code

    # é‡æ’åˆ—ä»¥åŒ¹é…é¢„æœŸæ ¼å¼
    columns = ["index_code", "con_code", "trade_date", "weight"]
    if "stock_name" in df_cons.columns:
        columns.append("stock_name")
    df_cons = df_cons[[c for c in columns if c in df_cons.columns]]

    df_cons.to_csv(weight_file, index=False, encoding="utf-8")
    print(f"âœ… æˆåˆ†è‚¡æƒé‡æ–‡ä»¶å·²æ›´æ–°: {weight_file} ({len(df_cons)} åªæˆåˆ†è‚¡)")
    return True


def fetch_missing_stocks(
    missing_codes: List[str],
    output_dir: Optional[Path] = None,
    start_date: str = "20250101",
    end_date: Optional[str] = None,
) -> pd.DataFrame:
    """è·å–ç¼ºå¤±è‚¡ç¥¨çš„å†å²æ•°æ®å¹¶åˆå¹¶åˆ°ç°æœ‰æ•°æ®

    Args:
        missing_codes: ç¼ºå¤±çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        start_date: èµ·å§‹æ—¥æœŸ YYYYMMDD
        end_date: ç»“æŸæ—¥æœŸï¼Œé»˜è®¤ä»Šå¤©

    Returns:
        è·å–åˆ°çš„æ–°æ•°æ® DataFrame
    """
    if not missing_codes:
        print("æ²¡æœ‰ç¼ºå¤±çš„è‚¡ç¥¨éœ€è¦è·å–")
        return pd.DataFrame()

    if output_dir is None:
        output_dir = Path(__file__).parent / "A_stock_data"

    if end_date is None:
        end_date = datetime.now().strftime("%Y%m%d")

    daily_file = output_dir / "daily_prices_sse_50.csv"

    print(f"ğŸ“Š è·å– {len(missing_codes)} åªç¼ºå¤±è‚¡ç¥¨çš„æ•°æ®: {missing_codes}")
    print(f"   æ—¥æœŸèŒƒå›´: {start_date} - {end_date}")

    source = create_data_source("akshare")
    df_new = source.get_stock_daily(missing_codes, start_date, end_date)

    if df_new.empty:
        print("âš ï¸  æœªè·å–åˆ°ç¼ºå¤±è‚¡ç¥¨çš„æ•°æ®")
        return df_new

    # åˆå¹¶åˆ°ç°æœ‰æ•°æ®
    if daily_file.exists():
        df_existing = pd.read_csv(daily_file)
        df_existing["trade_date"] = df_existing["trade_date"].astype(str)
        df_new["trade_date"] = df_new["trade_date"].astype(str)

        df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        df_combined = df_combined.drop_duplicates(
            subset=["ts_code", "trade_date"],
            keep="last"
        )
        df_combined = df_combined.sort_values(["ts_code", "trade_date"])
    else:
        df_combined = df_new

    df_combined.to_csv(daily_file, index=False, encoding="utf-8")
    print(f"âœ… å·²æ·»åŠ  {len(df_new)} æ¡è®°å½•åˆ° {daily_file}")
    print(f"   ç°æœ‰è‚¡ç¥¨æ•°: {df_combined['ts_code'].nunique()}")

    return df_new


def fix_missing_data(
    start_date: str = "20250101",
    output_dir: Optional[Path] = None,
    frequency: str = "daily",
) -> bool:
    """æ£€æµ‹å¹¶ä¿®å¤ç¼ºå¤±çš„è‚¡ç¥¨æ•°æ®

    è‡ªåŠ¨æ£€æµ‹ç¼ºå¤±çš„æˆåˆ†è‚¡å’ŒæŒä»“è‚¡ç¥¨ï¼Œå¹¶è·å–å…¶å†å²æ•°æ®ã€‚
    æ³¨æ„ï¼šè¢«å‰”é™¤çš„æˆåˆ†è‚¡å¦‚æœä»åœ¨æŒä»“ä¸­ï¼Œä¹Ÿéœ€è¦æ›´æ–°è¡Œæƒ…æ•°æ®ã€‚

    Args:
        start_date: èµ·å§‹æ—¥æœŸ
        output_dir: è¾“å‡ºç›®å½•
        frequency: æ•°æ®é¢‘ç‡ ("daily" æˆ– "hourly")

    Returns:
        True å¦‚æœä¿®å¤æˆåŠŸæˆ–æ— éœ€ä¿®å¤
    """
    from validate_data import DataValidator

    if output_dir is None:
        output_dir = Path(__file__).parent / "A_stock_data"

    print("\n" + "=" * 50)
    print("æ£€æµ‹ç¼ºå¤±æ•°æ®...")
    print("=" * 50)

    validator = DataValidator(output_dir)
    result = validator.validate(use_api=True, frequency=frequency)

    if result.error_message:
        print(f"âŒ éªŒè¯å¤±è´¥: {result.error_message}")
        return False

    if result.weight_file_outdated:
        print("æˆåˆ†è‚¡æƒé‡æ–‡ä»¶éœ€è¦æ›´æ–°ï¼Œæ­£åœ¨æ›´æ–°...")
        update_weight_file(output_dir=output_dir)

    # æ”¶é›†æ‰€æœ‰ç¼ºå¤±çš„è‚¡ç¥¨ï¼ˆæˆåˆ†è‚¡ + æŒä»“è‚¡ç¥¨ï¼‰
    all_missing = list(set(result.missing_stocks + result.missing_held_stocks))

    if not all_missing:
        print("âœ… æ²¡æœ‰ç¼ºå¤±çš„è‚¡ç¥¨æ•°æ®")
        return True

    # åˆ†ç±»æ˜¾ç¤º
    if result.missing_stocks:
        print(f"\nå‘ç° {len(result.missing_stocks)} åªç¼ºå¤±æˆåˆ†è‚¡:")
        for stock in result.missing_stocks:
            print(f"  - {stock}")

    if result.missing_held_stocks:
        print(f"\nå‘ç° {len(result.missing_held_stocks)} åªæŒä»“ç¼ºå¤±è¡Œæƒ…ï¼ˆå·²å‰”é™¤ä½†ä»æŒæœ‰ï¼‰:")
        for stock in result.missing_held_stocks:
            print(f"  - {stock}")

    print(f"\næ­£åœ¨è·å– {len(all_missing)} åªç¼ºå¤±è‚¡ç¥¨æ•°æ®...")
    df_new = fetch_missing_stocks(
        all_missing,
        output_dir=output_dir,
        start_date=start_date,
    )

    if not df_new.empty:
        print(f"âœ… æˆåŠŸè·å– {len(all_missing)} åªè‚¡ç¥¨çš„æ•°æ®")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†è‚¡ç¥¨æ•°æ®è·å–å¤±è´¥")
        return False


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="è·å– A è‚¡æ—¥çº¿æ•°æ®")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶å…¨é‡æ›´æ–°")
    parser.add_argument("--start-date", default="20251001", help="èµ·å§‹æ—¥æœŸ (YYYYMMDD)")
    parser.add_argument("--fix-missing", action="store_true", help="æ£€æµ‹å¹¶è·å–ç¼ºå¤±è‚¡ç¥¨æ•°æ®")
    parser.add_argument("--update-weights", action="store_true", help="æ›´æ–°æˆåˆ†è‚¡æƒé‡æ–‡ä»¶")
    args = parser.parse_args()

    # å¤‡ç”¨æˆåˆ†è‚¡æ–‡ä»¶è·¯å¾„
    fallback_path = Path(__file__).parent / "A_stock_data" / "sse_50_weight.csv"
    output_dir = Path(__file__).parent / "A_stock_data"

    # æ›´æ–°æƒé‡æ–‡ä»¶
    if args.update_weights:
        print("=" * 50)
        print("æ›´æ–°æˆåˆ†è‚¡æƒé‡æ–‡ä»¶...")
        print("=" * 50)
        update_weight_file(output_dir=output_dir)

    # è·å–æˆåˆ†è‚¡æ—¥çº¿æ•°æ®
    print("=" * 50)
    print("ä½¿ç”¨ AKShare è·å– A è‚¡æ•°æ®")
    print("=" * 50)

    df = get_daily_price_a_stock(
        index_code="000016.SH",
        daily_start_date=args.start_date,
        fallback_csv=fallback_path,
        force_update=args.force,
    )

    # è·å–æŒ‡æ•°æ—¥çº¿æ•°æ®
    print("\n" + "=" * 50)
    print("è·å–æŒ‡æ•°æ—¥çº¿æ•°æ®...")
    print("=" * 50)
    df_index = get_index_daily_data(
        index_code="000016.SH",
        start_date=args.start_date,
        force_update=args.force,
    )

    # æ£€æµ‹å¹¶ä¿®å¤ç¼ºå¤±æ•°æ®
    if args.fix_missing:
        fix_missing_data(
            start_date=args.start_date,
            output_dir=output_dir,
        )
